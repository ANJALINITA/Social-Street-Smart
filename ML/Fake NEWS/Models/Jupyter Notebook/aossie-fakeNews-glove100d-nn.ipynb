{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\visha\\anaconda3\\envs\\pyenv\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\visha\\anaconda3\\envs\\pyenv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\visha\\anaconda3\\envs\\pyenv\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\visha\\anaconda3\\envs\\pyenv\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\visha\\anaconda3\\envs\\pyenv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import  hstack\n",
    "from keras.layers import Input, concatenate,LSTM,Flatten, Dense,Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "# from handFeatures import hand_features, clean, get_tokenized_lemmas, remove_stopwords\n",
    "# from glove50_embedding import glove_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and tested\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    \n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        feats = feat_fn(headlines, bodies)\n",
    "        np.save(feature_file, feats)\n",
    "\n",
    "    return np.load(feature_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_overlap_features(features, headline, body):\n",
    "    # common word/ total word\n",
    "    clean_headline = clean(headline)\n",
    "    clean_body = clean(body)\n",
    "    clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "    clean_body = get_tokenized_lemmas(clean_body)\n",
    "    feature = len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))\n",
    "    features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def refuting_features(features,headline, body):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    X = []\n",
    "    clean_headline = clean(headline)\n",
    "    clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "    feature=0\n",
    "    for wrd in clean_headline:\n",
    "        if wrd in _refuting_words:\n",
    "            feature=1\n",
    "    features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def polarity_features(features,headline, body):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    X = []\n",
    "    clean_headline = clean(headline)\n",
    "    clean_body = clean(body)\n",
    "    features.append(calculate_polarity(clean_headline))\n",
    "    features.append(calculate_polarity(clean_body))\n",
    "    return features\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def append_chargrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in text_body[:100]:\n",
    "            grams_first_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    features.append(grams_first_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def append_ngrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def hand_features(headlines, bodies):\n",
    "    def binary_co_occurence(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in clean(headline).split(\" \"):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "            if headline_token in clean(body)[:255]:\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def binary_co_occurence_stops(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def count_grams(headline, body):\n",
    "        # Count how many times an n-gram of the title\n",
    "        # appears in the entire body, and intro paragraph\n",
    "\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = clean(headline)\n",
    "        features = []\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
    "        return features\n",
    "\n",
    "    def other_feature(headline, body):\n",
    "        features=[]\n",
    "        features= word_overlap_features(features, headline, body)\n",
    "        features= refuting_features(features, headline, body)\n",
    "        features= polarity_features(features,headline,body)\n",
    "        return features\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        lst= binary_co_occurence(headline, body) + binary_co_occurence_stops(headline, body) + count_grams(headline, body)+ other_feature(headline, body)\n",
    "        X.append(lst)\n",
    "\n",
    "\n",
    "    X =np.array(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x= hand_features(h,t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "#matrix1 = confusion_matrix(test_labels, prediction)\n",
    "#plot_confusion_matrix(cm=matrix1,target_names=['agree', 'disagree', 'discuss', 'unrelated'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 2/2 [00:00<00:00, 306.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.154305    0.40436     0.0659825  -0.15497201  0.20514999  0.80122498\n",
      "   0.29861498  0.65231    -0.55372001  0.77741    -0.59126     0.60309999\n",
      "  -0.35745649  0.13796     0.539635    0.33880549 -0.14214999 -0.31540001\n",
      "  -0.08566999  0.16207501 -0.044045   -0.532755   -0.03116    -0.04241499\n",
      "  -0.14816999  0.17823501  0.31223399 -0.4232837   0.108767    0.26135001\n",
      "   0.449995   -0.18367199 -0.136246   -0.033645    0.28059     0.189042\n",
      "   0.22823501 -0.06801499 -0.23754101 -0.42772001 -0.245415   -0.45503999\n",
      "   0.360415   -0.20957001  0.01927     0.15212     0.14541501 -1.17711997\n",
      "  -0.114215   -0.232645    1.04551497 -0.37946998  0.39599499  0.76536503\n",
      "  -0.58912998 -0.89366499 -0.4055775  -0.36572001  1.60865003  0.29840901\n",
      "   0.0815      0.14972501 -0.08866999 -0.25362504  1.05665499 -0.00167999\n",
      "   0.34857501  0.20403001 -0.53334001  0.13069999  0.022135    0.554295\n",
      "   0.57542001  0.422685    0.251855    0.71860501 -0.059782   -0.88275999\n",
      "  -0.36005999 -0.37763     0.30704501 -0.116795   -0.64842501 -0.69403499\n",
      "  -1.32929999 -0.08643     0.64852599 -0.48419499 -0.12489     0.03439501\n",
      "  -0.368875    0.08394001 -0.233655   -0.218245    0.58594     0.19320499\n",
      "   0.031295   -0.29155001 -0.276715   -0.00619   ]\n",
      " [ 0.0712525   0.21384575 -0.02095625  0.1823525   0.3518475   0.33181249\n",
      "  -0.25244001 -0.00788251 -0.09422001  0.32993251 -0.1006275   0.0997075\n",
      "  -0.13013732  0.0860275   0.1197175   0.24968849 -0.1310625   0.0045275\n",
      "  -0.191715    0.1053175  -0.0368815  -0.26586093 -0.0484215   0.24293\n",
      "   0.22030076  0.1494175   0.208036   -0.07817501 -0.252819   -0.02224749\n",
      "   0.096975    0.252779   -0.22292    -0.28599     0.118055    0.3051225\n",
      "   0.1536625   0.106825    0.1585445  -0.163917   -0.27986501 -0.05445899\n",
      "   0.24970075  0.05148475 -0.0288125   0.04854     0.37156    -0.29076248\n",
      "  -0.1031379  -0.03143251  0.464235   -0.31839774  0.09322499  0.64414251\n",
      "  -0.35978749 -0.61211253  0.04419625 -0.3965      0.91661251  0.14353\n",
      "  -0.08280749 -0.0274125  -0.0062825  -0.22243001  0.69321    -0.1595145\n",
      "   0.306695    0.13279828 -0.37245001  0.12989499 -0.2184325   0.09264175\n",
      "   0.1050055   0.018295    0.334785    0.1918375   0.134979   -0.32839\n",
      "  -0.17022999 -0.12362282  0.17781    -0.267605   -0.4384225  -0.1238125\n",
      "  -0.6738025   0.2953025   0.0825145  -0.310145    0.02491475 -0.37028\n",
      "  -0.27318858 -0.01508    -0.12031225 -0.07394325  0.31779001 -0.31591\n",
      "   0.0739475  -0.4267875   0.0063675   0.03169326]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure nltk stopwords are downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load GloVe embeddings\n",
    "filename = '../input/glove.6B.100d.txt'\n",
    "glove = {}\n",
    "with open(filename, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove[word] = coefs\n",
    "\n",
    "def pre_process(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and split text\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return text\n",
    "\n",
    "def make_embed(text):\n",
    "    feature = np.zeros((len(text), 100), dtype='object')\n",
    "    c = 0\n",
    "    for wrd in text:\n",
    "        if wrd in glove:\n",
    "            feature[c] = glove[wrd].reshape(1, 100)\n",
    "            c += 1\n",
    "    res = np.mean(feature, axis=0)\n",
    "    return res\n",
    "\n",
    "def glove_sentence_embeddings(texts):\n",
    "    embed = []\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        clean_text = pre_process(texts[i])\n",
    "        embed.append(make_embed(clean_text))\n",
    "    return np.array(embed, dtype='float')\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Another example sentence for embedding.\"\n",
    "]\n",
    "embeddings = glove_sentence_embeddings(texts)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFile=pd.read_csv('../input/train_merged.csv',encoding='latin-1')\n",
    "testFile=pd.read_csv('../input/test_merged.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBody= trainFile['articleBody'].tolist()\n",
    "trainHead= trainFile['Headline'].tolist()\n",
    "testBody= testFile['articleBody'].tolist()\n",
    "testHead= testFile['Headline'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49972it [10:01, 83.01it/s] \n",
      "25413it [04:45, 88.95it/s] \n"
     ]
    }
   ],
   "source": [
    "train_handF= hand_features(trainHead, trainBody)\n",
    "test_handF=hand_features(testHead, testBody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49972/49972 [1:48:50<00:00,  7.65it/s]   \n",
      "100%|██████████| 49972/49972 [03:10<00:00, 261.98it/s]\n",
      "100%|██████████| 25413/25413 [46:21<00:00,  9.14it/s]  \n",
      "100%|██████████| 25413/25413 [01:36<00:00, 263.81it/s]\n"
     ]
    }
   ],
   "source": [
    "trainBodyFeat= glove_sentence_embeddings(trainBody)\n",
    "trainHeadFeat= glove_sentence_embeddings(trainHead)\n",
    "testBodyFeat= glove_sentence_embeddings(testBody)\n",
    "testHeadFeat= glove_sentence_embeddings(testHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('glove100_trainBodyFeat',trainBodyFeat)\n",
    "np.save('glove100_trainHeadFeat',trainHeadFeat)\n",
    "np.save('glove100_testBodyFeat',testBodyFeat)\n",
    "np.save('glove100_testHeadFeat',testHeadFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeat= np.concatenate((trainHeadFeat, trainBodyFeat, train_handF), axis=-1)\n",
    "testFeat= np.concatenate((testHeadFeat, testBodyFeat, test_handF), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('trainFeatMatrix_glove_100',trainFeat)\n",
    "np.save('testFeatMatrix_glove_100',testFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 230) (25413, 230)\n"
     ]
    }
   ],
   "source": [
    "print(trainFeat.shape,testFeat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels= trainFile['Stance'].copy()\n",
    "test_labels= testFile['Stance'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">230</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">46,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">324</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m230\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │        \u001b[38;5;34m46,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │        \u001b[38;5;34m16,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m324\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,604</span> (244.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m62,604\u001b[0m (244.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,604</span> (244.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,604\u001b[0m (244.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = Input(shape=(trainFeat.shape[1],))\n",
    "lay1= Dense(200, activation= 'relu')(inp)\n",
    "lay1= Dropout(0.3)(lay1)\n",
    "lay2= Dense(80, activation= 'relu')(lay1)\n",
    "outp= Dense(4,activation='sigmoid')(lay2)\n",
    "model= Model(inputs=[inp], outputs=[outp])\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_FNC.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=r\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = pd.get_dummies(trainFile['Stance']).values\n",
    "testY = pd.get_dummies(testFile['Stance'] ).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3460/3460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.8624 - loss: 0.3837 - val_accuracy: 0.8333 - val_loss: 0.4478\n",
      "Epoch 2/5\n",
      "\u001b[1m  73/3460\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.8880 - loss: 0.3295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\visha\\anaconda3\\envs\\pyenv\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_acc available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3460/3460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.3029 - val_accuracy: 0.8407 - val_loss: 0.4338\n",
      "Epoch 3/5\n",
      "\u001b[1m3460/3460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8959 - loss: 0.2812 - val_accuracy: 0.8405 - val_loss: 0.4391\n",
      "Epoch 4/5\n",
      "\u001b[1m3460/3460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8983 - loss: 0.2739 - val_accuracy: 0.8543 - val_loss: 0.4390\n",
      "Epoch 5/5\n",
      "\u001b[1m3460/3460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9036 - loss: 0.2576 - val_accuracy: 0.8533 - val_loss: 0.4183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ea1d8a64b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trainFeat],[trainY],epochs=5,batch_size=13,verbose=1, validation_split=0.1, callbacks=callbacks_list, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok= Tokenizer()\n",
    "model_json = model.to_json()\n",
    "with open(\"../output/glove/fakenews.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"../output/glove/fakenews.weights.h5\")\n",
    "\n",
    "with open('../output/glove/fakenews.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m795/795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.3188\n"
     ]
    }
   ],
   "source": [
    "scor, acc = model.evaluate([testFeat],[testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8532640933990479, 0.4865679442882538)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc,scor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
